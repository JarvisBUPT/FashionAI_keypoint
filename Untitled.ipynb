{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# %load reader_train_csv.py\n",
    "import csv\n",
    "from itertools import islice\n",
    "\n",
    "\"\"\"\n",
    "    该文件是一个测试文件，测试怎么使用读取和写入csv文件\n",
    "\"\"\"\n",
    "data_path = \"/home/sk39/workspace/dataset/tianchi_clothes/train/Annotations/annotations.csv\"\n",
    "test_path = \"train_1.csv\"\n",
    "f = open(test_path, \"r\")\n",
    "keys = f.readline().split(',')  # 读取第一行的内容\n",
    "a = False\n",
    "b= True\n",
    "if not a and b:\n",
    "    print(\"1\")\n",
    "# print(keys)\n",
    "# f.close()\n",
    "# imgs_dict = {}\n",
    "# data_table = []\n",
    "# with open(test_path, \"r\") as f:\n",
    "#     for value in islice(f, 1, None):\n",
    "#         v = value.split(',')\n",
    "#         print(v)\n",
    "\n",
    "# with open(test_path, \"r\") as f:\n",
    "#     reader = csv.DictReader(f)\n",
    "#     rows = [row for row in reader]\n",
    "# print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training_txt_file': 'dataset.txt', 'img_directory': '/home/sk39/workspace/dataset/mpii/images', 'img_directory_win': './', 'img_size': 256, 'hm_size': 64, 'num_joints': 16, 'remove_joints': None, 'joint_list': ['r_anckle', 'r_knee', 'r_hip', 'l_hip', 'l_knee', 'l_anckle', 'pelvis', 'thorax', 'neck', 'head', 'r_wrist', 'r_elbow', 'r_shoulder', 'l_shoulder', 'l_elbow', 'l_wrist'], 'name': 'hg_refined_200', 'nfeats': 256, 'nstacks': 4, 'nmodules': 1, 'tiny': False, 'nlow': 4, 'dropout_rate': 0.2, 'mcam': False, 'batch_size': 4, 'nepochs': 200, 'epoch_size': 1000, 'learning_rate': 0.00025, 'learning_rate_decay': 0.96, 'decay_step': 2000, 'weighted_loss': False, 'valid_iteration': 10, 'log_dir_train': '/home/sk39/workspace/zhoudu/hourglasstensorlfow/hourglass_saver/', 'log_dir_test': '/home/sk39/workspace/zhoudu/hourglasstensorlfow/hourglass_saver/', 'saver_step': 500, 'saver_directory': ''}\n",
      "CREATE MODEL:\n",
      "---Inputs : Done (0 sec.)\n",
      "---Graph : Done (7 sec.)\n",
      "---Loss : Done (0 sec.)\n",
      "---Acc : Done (1 sec.)\n",
      "---LR : Done (0 sec.)\n",
      "---Optim : Done (0 sec.)\n",
      "---Minimizer : Done (9 sec.)\n",
      "---Init : Done (0 sec.)\n",
      "Model created (18 sec.)\n",
      "Graph Generated in  18  sec.\n",
      "Session initialization\n",
      "Sess initialized in 0 sec.\n",
      "Loading Trained Model\n",
      "INFO:tensorflow:Restoring parameters from hg_refined_200\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Dst tensor is not initialized.\n\t [[Node: Session/save/RestoreV2/_3221 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_3228_Session/save/RestoreV2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Session/save/RestoreV2/_1050 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_1056_Session/save/RestoreV2\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Session/save/RestoreV2:906)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[Node: Session/save/RestoreV2/_3221 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_3228_Session/save/RestoreV2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Session/save/RestoreV2/_1050 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_1056_Session/save/RestoreV2\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Session/save/RestoreV2:906)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bbc3a9723502>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0minf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'config_test.cfg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hg_refined_200'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     dataset = DataGenerator(params['joint_list'], params['img_directory'], params['training_txt_file'],\n\u001b[1;32m    273\u001b[0m                             remove_joints=params['remove_joints'])\n",
      "\u001b[0;32m<ipython-input-10-bbc3a9723502>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config_file, model, yoloModel)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINKS_JOINTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myolo_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_yolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myoloModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/zhoudu/FashionAI_keypoint/predictClass.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_joint_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'joint_tensor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/zhoudu/FashionAI_keypoint/hourglass_tiny.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    231\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading Trained Model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m                     \u001b[0;31m# try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m#   self.saver.restore(self.Session, load)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1754\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1755\u001b[0;31m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1756\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1757\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[Node: Session/save/RestoreV2/_3221 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_3228_Session/save/RestoreV2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Session/save/RestoreV2/_1050 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_1056_Session/save/RestoreV2\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Session/save/RestoreV2:906)]]"
     ]
    }
   ],
   "source": [
    "# %load inference.py\n",
    "\"\"\"\n",
    "Deep Human Pose Estimation\n",
    "\n",
    "Project by Walid Benbihi\n",
    "MSc Individual Project\n",
    "Imperial College\n",
    "Created on Mon Sep  4 18:11:46 2017\n",
    "\n",
    "@author: Walid Benbihi\n",
    "@mail : w.benbihi(at)gmail.com\n",
    "@github : https://github.com/wbenbihi/hourglasstensorlfow/\n",
    "\n",
    "Abstract:\n",
    "    This python code creates a Stacked Hourglass Model\n",
    "    (Credits : A.Newell et al.)\n",
    "    (Paper : https://arxiv.org/abs/1603.06937)\n",
    "\n",
    "    Code translated from 'anewell' github\n",
    "    Torch7(LUA) --> TensorFlow(PYTHON)\n",
    "    (Code : https://github.com/anewell/pose-hg-train)\n",
    "\n",
    "    Modification are made and explained in the report\n",
    "    Goal : Achieve Real Time detection (Webcam)\n",
    "    ----- Modifications made to obtain faster results (trade off speed/accuracy)\n",
    "\n",
    "    This work is free of use, please cite the author if you use it!\n",
    "\n",
    "\"\"\"\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('./')\n",
    "\n",
    "from hourglass_tiny import HourglassModel\n",
    "from time import time, clock\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "from train_launcher import process_config\n",
    "import cv2\n",
    "from predictClass import PredictProcessor\n",
    "from yolo_net import YOLONet\n",
    "from datagen import DataGenerator\n",
    "import config as cfg\n",
    "from filters import VideoFilters\n",
    "\n",
    "\n",
    "class Inference():\n",
    "    \"\"\" Inference Class\n",
    "    Use this file to make your prediction\n",
    "    Easy to Use\n",
    "    Images used for inference should be RGB images (int values in [0,255])\n",
    "    Methods:\n",
    "        webcamSingle    : Single Person Pose Estimation on Webcam Stream\n",
    "        webcamMultiple  : Multiple Person Pose Estimation on Webcam Stream\n",
    "        webcamPCA       : Single Person Pose Estimation with reconstruction error (PCA)\n",
    "        webcamYOLO      : Object Detector\n",
    "        predictHM       : Returns Heat Map for an input RGB Image\n",
    "        predictJoints   : Returns joint's location (for a 256x256 image)\n",
    "        pltSkeleton     : Plot skeleton on image\n",
    "        runVideoFilter  : SURPRISE !!!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_file='config.cfg', model='hg_refined_tiny_200', yoloModel='YOLO_small.ckpt'):\n",
    "        \"\"\" Initilize the Predictor\n",
    "        Args:\n",
    "            config_file \t \t: *.cfg file with model's parameters\n",
    "            model \t \t \t \t: *.index file's name. (weights to load)\n",
    "            yoloModel \t \t    : *.ckpt file (YOLO weights to load)\n",
    "        \"\"\"\n",
    "        t = time()\n",
    "        params = process_config(config_file)\n",
    "        self.predict = PredictProcessor(params)\n",
    "        self.predict.color_palette()\n",
    "        self.predict.LINKS_JOINTS()\n",
    "        self.predict.model_init()\n",
    "        self.predict.load_model(load=model)\n",
    "        self.predict.yolo_init()\n",
    "        self.predict.restore_yolo(load=yoloModel)\n",
    "        self.predict._create_prediction_tensor()\n",
    "        self.filter = VideoFilters()\n",
    "        print('Done: ', time() - t, ' sec.')\n",
    "\n",
    "    # -------------------------- WebCam Inference-------------------------------\n",
    "    def webcamSingle(self, thresh=0.2, pltJ=True, pltL=True):\n",
    "        \"\"\" Run Single Pose Estimation on Webcam Stream\n",
    "        Args :\n",
    "            thresh  : Joint Threshold\n",
    "            pltJ    : (bool) True to plot joints\n",
    "            pltL    : (bool) True to plot limbs\n",
    "        \"\"\"\n",
    "        self.predict.hpeWebcam(thresh=thresh, plt_j=pltJ, plt_l=pltL, plt_hm=False, debug=False)\n",
    "\n",
    "    def webcamMultiple(self, thresh=0.2, nms=0.5, resolution=800, pltL=True, pltJ=True, pltB=True, isolate=False):\n",
    "        \"\"\" Run Multiple Pose Estimation on Webcam Stream\n",
    "        Args:\n",
    "            thresh      : Joint Threshold\n",
    "            nms         : Non Maxima Suppression Threshold\n",
    "            resolution  : Stream Resolution\n",
    "            pltJ        : (bool) True to plot joints\n",
    "            pltL        : (bool) True to plot limbs\n",
    "            pltB        : (bool) True to plot bounding boxes\n",
    "            isolate     : (bool) True to show isolated skeletons\n",
    "        \"\"\"\n",
    "        self.predict.mpe(j_thresh=thresh, nms_thresh=nms, plt_l=pltL, plt_j=pltJ, plt_b=pltB, img_size=resolution,\n",
    "                         skeleton=isolate)\n",
    "\n",
    "    def webcamPCA(self, n=5, matrix='p4frames.mat'):\n",
    "        \"\"\" Run Single Pose Estimation with Error Reconstruction on Webcam Stream\n",
    "        Args:\n",
    "            n       : Number of dimension to keep before reconstruction\n",
    "            matrix  : MATLAB eigenvector matrix to load\n",
    "        \"\"\"\n",
    "        self.predict.reconstructACPVideo(load=matrix, n=n)\n",
    "\n",
    "    def webcamYOLO(self):\n",
    "        \"\"\" Run Object Detection on Webcam Stream\n",
    "        \"\"\"\n",
    "        cam = cv2.VideoCapture(0)\n",
    "        return self.predict.camera_detector(cam, wait=0, mirror=True)\n",
    "\n",
    "    # ----------------------- Heat Map Prediction ------------------------------\n",
    "\n",
    "    def predictHM(self, img):\n",
    "        \"\"\" Return Sigmoid Prediction Heat Map\n",
    "        Args:\n",
    "            img : Input Image -shape=(256x256x3) -value= uint8 (in [0, 255])\n",
    "        \"\"\"\n",
    "        return self.predict.pred(self, img / 255, debug=False, sess=None)\n",
    "\n",
    "    # ------------------------- Joint Prediction -------------------------------\n",
    "\n",
    "    def predictJoints(self, img, mode='cpu', thresh=0.2):\n",
    "        \"\"\" Return Joint Location\n",
    "        /!\\ Location with respect to 256x256 image\n",
    "        Args:\n",
    "            img     : Input Image -shape=(256x256x3) -value= uint8 (in [0, 255])\n",
    "            mode    : 'cpu' / 'gpu' Select a mode to compute joints' location\n",
    "            thresh  : Joint Threshold\n",
    "        \"\"\"\n",
    "        SIZE = False\n",
    "        if len(img.shape) == 3:\n",
    "            batch = np.expand_dims(img, axis=0)\n",
    "            SIZE = True\n",
    "        elif len(img.shape) == 4:\n",
    "            batch = np.copy(img)\n",
    "            SIZE = True\n",
    "        print(batch.shape)\n",
    "        if SIZE:\n",
    "            if mode == 'cpu':\n",
    "                return self.predict.joints_pred_numpy(batch / 255, coord='img', thresh=thresh, sess=None)\n",
    "            elif mode == 'gpu':\n",
    "                return self.predict.joints_pred(batch / 255, coord='img', debug=False, sess=None)\n",
    "            else:\n",
    "                print(\"Error : Mode should be 'cpu'/'gpu'\")\n",
    "        else:\n",
    "            print('Error : Input is not a RGB image nor a batch of RGB images')\n",
    "\n",
    "    # ----------------------------- Plot Skeleton ------------------------------\n",
    "\n",
    "    def pltSkeleton(self, img, thresh, pltJ, pltL):\n",
    "        \"\"\" Return an image with plotted joints and limbs\n",
    "        Args:\n",
    "            img     : Input Image -shape=(256x256x3) -value= uint8 (in [0, 255])\n",
    "            thresh  : Joint Threshold\n",
    "            pltJ    : (bool) True to plot joints\n",
    "            pltL    : (bool) True to plot limbs\n",
    "        \"\"\"\n",
    "        return self.predict.pltSkeleton(img, thresh=thresh, pltJ=pltJ, pltL=pltL, tocopy=True, norm=True)\n",
    "\n",
    "    # -------------------------- Video Processing ------------------------------\n",
    "\n",
    "    def processVideo(self, source=None, outfile=None, thresh=0.2, nms=0.5, codec='DIVX', pltJ=True, pltL=True,\n",
    "                     pltB=True, show=False):\n",
    "        \"\"\" Run Multiple Pose Estimation on Video Footage\n",
    "        Args:\n",
    "            source      : Input Footage\n",
    "            outfile     : File to Save\n",
    "            thesh       : Joints Threshold\n",
    "            nms         : Non Maxima Suppression Threshold\n",
    "            codec       : Codec to use\n",
    "            pltJ        : (bool) True to plot joints\n",
    "            pltL        : (bool) True to plot limbs\n",
    "            pltB        : (bool) True to plot bounding boxes\n",
    "            show        : (bool) Show footage during processing\n",
    "        \"\"\"\n",
    "        return self.predict.videoDetection(src=source, outName=outfile, codec=codec, j_thresh=thresh, nms_thresh=nms,\n",
    "                                           show=show, plt_j=pltJ, plt_l=pltL, plt_b=pltB)\n",
    "\n",
    "    # -------------------------- Process Stream --------------------------------\n",
    "\n",
    "    def centerStream(self, img):\n",
    "        img = cv2.flip(img, 1)\n",
    "        img[:,\n",
    "        self.predict.cam_res[1] // 2 - self.predict.cam_res[0] // 2:self.predict.cam_res[1] // 2 + self.predict.cam_res[\n",
    "                                                                                                       0] // 2]\n",
    "        img_hg = cv2.resize(img, (256, 256))\n",
    "        img_res = cv2.resize(img, (800, 800))\n",
    "        img_hg = cv2.cvtColor(img_hg, cv2.COLOR_BGR2RGB)\n",
    "        return img_res, img_hg\n",
    "\n",
    "    def plotLimbs(self, img_res, j):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        for i in range(len(self.predict.links)):\n",
    "            l = self.predict.links[i]['link']\n",
    "            good_link = True\n",
    "            for p in l:\n",
    "                if np.array_equal(j[p], [-1, -1]):\n",
    "                    good_link = False\n",
    "            if good_link:\n",
    "                pos = self.predict.givePixel(l, j)\n",
    "                cv2.line(img_res, tuple(pos[0])[::-1], tuple(pos[1])[::-1], self.predict.links[i]['color'][::-1],\n",
    "                         thickness=5)\n",
    "\n",
    "    # -----------------------------  Filters -----------------------------------\n",
    "\n",
    "    def runVideoFilter(self, debug=False):\n",
    "        \"\"\" WORK IN PROGRESS\n",
    "        Mystery Function\n",
    "        \"\"\"\n",
    "        thresh = 0.2\n",
    "        cam = cv2.VideoCapture(self.predict.src)\n",
    "        self.filter.activated_filters = [0] * self.filter.num_filters\n",
    "        while True:\n",
    "            t = time()\n",
    "            ret_val, img = cam.read()\n",
    "            img_res, img_hg = self.centerStream(img)\n",
    "            hg = self.predict.pred(img_hg / 255)\n",
    "            j = np.ones(shape=(self.predict.params['num_joints'], 2)) * -1\n",
    "            for i in range(len(j)):\n",
    "                idx = np.unravel_index(hg[0, :, :, i].argmax(), (64, 64))\n",
    "                if hg[0, idx[0], idx[1], i] > thresh:\n",
    "                    j[i] = np.asarray(idx) * 800 / 64\n",
    "                    if debug:\n",
    "                        cv2.circle(img_res, center=tuple(j[i].astype(np.int))[::-1], radius=5,\n",
    "                                   color=self.predict.color[i][::-1], thickness=-1)\n",
    "            if debug:\n",
    "                print(j[9])\n",
    "                self.plotLimbs(img_res, j)\n",
    "            X = j.reshape((32, 1), order='F')\n",
    "            _, angles = self.filter.angleAdir(X)\n",
    "            for f in range(len(self.filter.existing_filters)):\n",
    "                if np.sum(self.filter.activated_filters) > 0:\n",
    "                    break\n",
    "                self.filter.activated_filters[f] = int(eval('self.filter.' + self.filter.existing_filters[f])(angles))\n",
    "            filter_to_activate = np.argmax(self.filter.activated_filters)\n",
    "            if self.filter.activated_filters[0] > 0:\n",
    "                img_res = eval('self.filter.' + self.filter.filter_func[filter_to_activate])(img_res, j)\n",
    "            fps = 1 / (time() - t)\n",
    "            cv2.putText(img_res, str(self.filter.activated_filters[0]) + '- FPS: ' + str(fps)[:4], (60, 40), 2, 2,\n",
    "                        (0, 0, 0), thickness=2)\n",
    "            cv2.imshow('stream', img_res)\n",
    "            if cv2.waitKey(1) == 27:\n",
    "                print('Stream Ended')\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "        cv2.destroyAllWindows()\n",
    "        cam.release()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    name = os.name\n",
    "    if name == 'nt':\n",
    "        config_file = 'config_win.cfg'\n",
    "    else:\n",
    "        config_file = 'config_test.cfg'\n",
    "    params = process_config(config_file)\n",
    "    print(params)\n",
    "    inf = Inference('config_test.cfg', 'hg_refined_200')\n",
    "    dataset = DataGenerator(params['joint_list'], params['img_directory'], params['training_txt_file'],\n",
    "                            remove_joints=params['remove_joints'])\n",
    "    img = dataset.open_img('000033016.jpg')\n",
    "    img1 = cv2.resize(img, (256, 256))\n",
    "    print(img.shape)\n",
    "    print(img1.shape)\n",
    "    pJ=inf.predictJoints(img1)\n",
    "    print(\"pj = \", pJ)\n",
    "    print(pJ.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataSetHG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-89fb110f742a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %load config_test.cfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mDataSetHG\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtraining_txt_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dataset.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg_directory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'/home/sk39/workspace/dataset/mpii/images'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg_directory_win\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'./'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataSetHG' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
